{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Extracting data from the UKGWA.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPBjjWMsCxqSMNZskscTnmK",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nationalarchives/UKGWA-computational-access/blob/main/Extracting_data_from_the_UKGWA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXOdnNLzO1fK"
      },
      "source": [
        "## Load library functions and data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTza1RbNLG25"
      },
      "source": [
        "import sys\n",
        "import os\n",
        "import shutil\n",
        "from matplotlib import pyplot\n",
        "from matplotlib.pyplot import figure\n",
        "from operator import itemgetter\n",
        "import datetime\n",
        "from gensim.summarization import summarizer\n",
        "# Function to convert snapshot timestamp to a date\n",
        "snap_to_date = lambda d : datetime.datetime.strptime(str(d), '%Y%m%d%H%M%S') # %I:%M%p')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3URYl6-UL-95",
        "outputId": "3317e7ca-18da-477c-874b-545dbc2cd248"
      },
      "source": [
        "if os.path.isdir('AURA-Article'):\n",
        "    shutil.rmtree('AURA-Article')\n",
        "!git clone https://github.com/mark-bell-tna/AURA-Article.git\n",
        "sys.path.insert(0, 'AURA-Article/Code')\n",
        "data_folder = \"./AURA-Article/Data/\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'AURA-Article'...\n",
            "remote: Enumerating objects: 110, done.\u001b[K\n",
            "remote: Counting objects: 100% (110/110), done.\u001b[K\n",
            "remote: Compressing objects: 100% (75/75), done.\u001b[K\n",
            "remote: Total 110 (delta 44), reused 93 (delta 32), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (110/110), 14.94 MiB | 38.73 MiB/s, done.\n",
            "Resolving deltas: 100% (44/44), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmz7oeoQMBvZ"
      },
      "source": [
        "from disco_search import DiscoSearch\n",
        "from ukgwa_textindex import UKGWATextIndex\n",
        "from ukgwa_cdx_indexer import TemporalIndexer\n",
        "from ukgwa_index import UKGWAIndex\n",
        "from ukgwa_url import UKGWAurl\n",
        "from bs4 import BeautifulSoup\n",
        "import requests"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UuYSYaGI2bS2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c870d681-6570-4878-f49d-9e1dae5eb52f"
      },
      "source": [
        "!pip install boilerpy3\n",
        "import boilerpy3\n",
        "from boilerpy3 import extractors"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: boilerpy3 in /usr/local/lib/python3.7/dist-packages (1.0.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JHoVWTjiGky"
      },
      "source": [
        "## Extracting links from archived web pages\n",
        "\n",
        "Opening and extracting from a web page is made easy by the Python library BeautifulSoup. It converts an HTML page into structural elements and provides functions for navigating the structure and extracting elements by type.\n",
        "One element which is of interest is the hyperlink. This consists of a descriptive label and address of a web page. By extracting all of the links from a page, or a set of pages, we can create the data to perform network analysis.\n",
        "Before doing that we need to be aware of the varying formats of urls which appear on a page.\n",
        "\n",
        "Change the ex_id variable in the following code to print out the links from different pages.\n",
        "\n",
        "The Environment Agency page has two formats of link: snapshot + page, or ./page. The latter is known as relative addressing which means it refers to files which are in a sub-folder of the current page address. The former we will call snapshot format.\n",
        "\n",
        "The Salt home page also includes two formats: this time the relative addresses do not include the ./. There are also two links to a page in a different domain (food.gov.uk). In this case the address not only includes the snapshot but also has the prefix of the web archive domain. We will refer to this a complete address. An address in this format can be immediately navigated to (or crawled) without any pre-processing of the address.\n",
        "\n",
        "The Office for Rail Regulation (orr.gov.uk) links are almost exclusively in the format of a complete address with only a few in snapshot format."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XDxxk8DhOcT1",
        "outputId": "5611cdf7-2849-43c1-8767-247f5b7ea1a0"
      },
      "source": [
        "ex_id = 3  # Change this to a number between 1 and 6 to change the example\n",
        "\n",
        "ukgwa_prefix = \"https://webarchive.nationalarchives.gov.uk/\"\n",
        "examples = [[\"http://www.environment-agency.gov.uk:80\",\"19961104034437\"],\n",
        "            [\"http://www.salt.gov.uk\", \"20090810121540\"],\n",
        "            [\"http://www.ukbms.org\", \"20140402194840\"],\n",
        "            [\"http://www.esero.org.uk\", \"20140508193217\"],\n",
        "            [\"http://www.professionalstandards.org.uk\", \"20160506161604\"],\n",
        "            [\"http://www.investorsinpeople.co.uk/Pages/Home.aspx\", \"20120418183519\"]]\n",
        "\n",
        "ex = examples[ex_id-1]\n",
        "page = requests.get(ukgwa_prefix + ex[1] + \"/\" + ex[0])\n",
        "soup = BeautifulSoup(page.content, 'html.parser')\n",
        "\n",
        "print(\"Links from\", ex[1] + \"/\" + ex[0])\n",
        "links = soup.find_all('a')\n",
        "for ln in links:\n",
        "    if ln.has_attr('href'):\n",
        "        print(\"\\t\",ln['href'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Links from 20140402194840/http://www.ukbms.org\n",
            "\t #NavigationMenu_SkipLink\n",
            "\t Default.aspx\n",
            "\t involved.aspx\n",
            "\t wcbs.aspx\n",
            "\t indicators.aspx\n",
            "\t KeyFindings.aspx\n",
            "\t news.aspx\n",
            "\t reportsAndPublications.aspx\n",
            "\t resources.aspx\n",
            "\t Specieslist.aspx\n",
            "\t map.aspx\n",
            "\t About.aspx\n",
            "\t Methods.aspx\n",
            "\t Obtaining.aspx\n",
            "\t Links.aspx\n",
            "\t Contacts.aspx\n",
            "\t #SiteMapDataProvider_SkipLink\n",
            "\t wcbs.aspx\n",
            "\t about.aspx\n",
            "\t Methods.aspx\n",
            "\t Obtaining.aspx\n",
            "\t https://webarchive.nationalarchives.gov.uk/20140402194840/http://www.butterfly-conservation.org/bnm/atlas/index.html\n",
            "\t https://webarchive.nationalarchives.gov.uk/20140402194840/http://www.naturebureau.co.uk/shop/books/StateofButterflies.html\n",
            "\t docs/reports/2013/Country-level Summary of changes Table 2013.pdf\n",
            "\t docs/reports/2013/UK Summary of changes Table 2013.pdf\n",
            "\t Downloads\\Wider_Countryside\\Newsletter_WCBS2013.pdf\n",
            "\t downloads/National Butterfly Recorders' Meeting 2014 Programme.pdf\n",
            "\t https://webarchive.nationalarchives.gov.uk/20140402194840/http://butterfly-conservation.org/244-4890/national-butterfly-recorders-meeting.html\n",
            "\t https://webarchive.nationalarchives.gov.uk/20140402194840/http://www.ukbms.org/mydata\n",
            "\t KeyFindings.aspx\n",
            "\t https://webarchive.nationalarchives.gov.uk/20140402194840/http://www.ceh.ac.uk/\n",
            "\t https://webarchive.nationalarchives.gov.uk/20140402194840/http://www.butterfly-conservation.org/\n",
            "\t https://webarchive.nationalarchives.gov.uk/20140402194840/http://www.jncc.gov.uk/\n",
            "\t https://webarchive.nationalarchives.gov.uk/20140402194840/https://www.gov.uk/defra\n",
            "\t https://webarchive.nationalarchives.gov.uk/20140402194840/http://www.ccw.gov.uk/\n",
            "\t https://webarchive.nationalarchives.gov.uk/20140402194840/http://www.naturalengland.org.uk/\n",
            "\t https://webarchive.nationalarchives.gov.uk/20140402194840/http://www.forestry.gov.uk/\n",
            "\t https://webarchive.nationalarchives.gov.uk/20140402194840/http://www.snh.org.uk/\n",
            "\t https://webarchive.nationalarchives.gov.uk/20140402194840/http://www.BTO.org/\n",
            "\t wider_countryside_reporting_visit.aspx\n",
            "\t https://webarchive.nationalarchives.gov.uk/20140402194840/http://www.ceh.ac.uk/\n",
            "\t https://webarchive.nationalarchives.gov.uk/20140402194840/http://www.butterfly-conservation.org\n",
            "\t https://webarchive.nationalarchives.gov.uk/20140402194840/http://www.jncc.gov.uk/\n",
            "\t https://webarchive.nationalarchives.gov.uk/20140402194840/http://www.defra.gov.uk/\n",
            "\t https://webarchive.nationalarchives.gov.uk/20140402194840/http://www.brc.ac.uk/\n",
            "\t https://webarchive.nationalarchives.gov.uk/20140402194840/http://www.google.com/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRTItrkenZNU"
      },
      "source": [
        "What the preceding code has demonstrated is that a certain amount of pre-processing is necessary to resolve link addresses to valid web archive addresses (complete addresses). The solution is simple but is a step that users should be aware of. The following steps should be taken to standardise addresses:\n",
        "\n",
        "1. Complete addresses: leave as they are\n",
        "2. Snapshot addresses: prefix with \"https://webarchive.nationalarchives.gov.uk/\"\n",
        "3. Relative addresses: prefix with the complete address of the page containing the link which must also end in a \"/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iW5QbU9Bo_Eu"
      },
      "source": [
        "The next thing to be aware of is that the snapshot timestamp in a web archive URL does not necessarily point to an existing page. The timestamp of a page refers to the time it was crawled which may have occurred seconds or even years after the timestamp in a hyperlink to that page.\n",
        "\n",
        "We can use the CDX data to further analyse this. From the Environment Agency homepage we see that their State of the Environment report was captured 1154 seconds (19 minutes) later than the link address."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-L82bWL6vzuW",
        "outputId": "7fab9bd7-9667-4977-ff7b-140aa10554e4"
      },
      "source": [
        "T = TemporalIndexer()\n",
        "url = \"http://www.environment-agency.gov.uk:80/s-enviro.html\"\n",
        "snapshot = 19961104034437\n",
        "T.add_entry(url)\n",
        "nearest_snapshot = snap_to_date(T.get_field(url, 'CDX').nearest_to(19961104034437))\n",
        "(nearest_snapshot - snap_to_date(snapshot))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "datetime.timedelta(seconds=1154)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIHe5MbczGxq"
      },
      "source": [
        "In constrast, the \"Who we are\" page was first captured over 467 later than the link address states. This is an extreme example but makes the point that whether a page linked to is crawled or not depends on both the depth of the crawl and the success of crawling an individual page."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nOyMe8uByDF3",
        "outputId": "9fb08e37-ae10-4a6a-be28-1859ce040f01"
      },
      "source": [
        "T = TemporalIndexer()\n",
        "url = \"http://www.environment-agency.gov.uk:80/who.html\"\n",
        "snapshot = 19961104034437\n",
        "T.add_entry(url)\n",
        "nearest_snapshot = snap_to_date(T.get_field(url, 'CDX').nearest_to(19961104034437))\n",
        "(nearest_snapshot - snap_to_date(snapshot))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "datetime.timedelta(days=467, seconds=26444)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fpyvkc_18Ayw"
      },
      "source": [
        "## Extracting content from archived web pages\n",
        "\n",
        "One challenge with working with HTML pages is extracting text from those pages. Using the same examples from before, we can use the BeautifulSoup library to extract text from each page.\n",
        "\n",
        "It should be immediately obvious that this is messy as the text also includes a javascript program which is used to render the page in the web archive interface."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "HjGignTc2ZV5",
        "outputId": "22b01809-30fc-48a9-8bd5-1da0cb218b01"
      },
      "source": [
        "ex_id = 1\n",
        "ex = examples[ex_id-1]\n",
        "page = requests.get(ukgwa_prefix + ex[1] + \"/\" + ex[0])\n",
        "soup = BeautifulSoup(page.content, 'html.parser')\n",
        "soup.get_text()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n\\n\\n\\n    // This block sets up a series of server-defined variables for use within Wombat\\n    wbinfo = {\\n        url           : \"http://www.environment-agency.gov.uk:80/\",\\n        timestamp     : \"19961104034437\",\\n        request_ts    : \"19961104034437\",\\n        prefix        : decodeURI(\"https://webarchive.nationalarchives.gov.uk/\"),\\n        mod           : \"\",\\n        origurl       : \"19961104034437/www.environment-agency.gov.uk:80/\",\\n        type          : \"replay\",\\n        top_url       : \"https://webarchive.nationalarchives.gov.uk/19961104034437tf_/http://www.environment-agency.gov.uk:80/\",\\n        is_framed     : true,\\n        is_live       : false,\\n        coll          : \"\",\\n        proxy_magic   : \"\",\\n        static_prefix : \"https://webarchive.nationalarchives.gov.uk/static/__pywb\"\\n    };\\n\\n\\n \\n \\n\\n \\n\\n\\n    // Since we\\'re in rewrite mode, add a few more variables for later use\\n    wbinfo.wombat_ts = \"19961104034437\";\\n    wbinfo.wombat_scheme = \"http\";\\n    wbinfo.wombat_host = \"www.environment-agency.gov.uk:80\";\\n    wbinfo.wombat_sec = \"847079077\";\\n    wbinfo.wombat_opts = {};\\n\\n    // If we don\\'t already have a Wombat instance in the page, set one up\\n    if (window && window._WBWombat && !window._wb_js_inited && !window._wb_wombat) {\\n        window._wb_wombat = new _WBWombat(wbinfo);\\n    }\\n\\n    // If all our relevant information exists, set the banner details\\n    if (wbinfo && wbinfo.timestamp) {\\n        // Create our prettified date for the banner\\n        ts_new_str = _wb_js.ts_to_pretty_date(wbinfo.timestamp);\\n\\n        // Get our text elements to setup\\n        var mobileTextElements = window.top.document.getElementsByClassName(\\'mw-mobile-text\\');\\n        var desktopTextElements = window.top.document.getElementsByClassName(\\'mw-desktop-text\\');\\n        \\n        if(desktopTextElements.length > 0) {\\n            console.log(\"[MW Head Insert] Set banner text for desktop.\");\\n            desktopTextElements[0].innerHTML = ts_new_str;\\n        }\\n\\n        if(mobileTextElements.length > 0) {\\n            console.log(\"[MW Head Insert] Set banner text for mobile.\");\\n            mobileTextElements[0].innerHTML = ts_new_str;\\n        }\\n  }\\n\\n\\n if (window._wb_js) { _wb_js.load(); }\\n\\nEnvironment Agency Home Page\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nWelcome to the Environment Agency of England and Wales Web Site.\\n\\nA text only version of this site is available.\\nA key feature of this site is our State of the Environment report: this is a snapshot look at the pressures on the environment and how the quality of the environment has changed over the last twenty-five or so years.\\n\\n\\nThis web site will be regularly updated and enhanced to provide comprehensive information and advice on the protection and management of the environment.  Look in the information and resource pages for details on how to contact us.\\n\\n\\nYou can also request copies of our  publications and leaflets on line and review the educational  services we will be offering.  And, of course we encourage you to give us your comments \\non any aspect of the Agency. \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nTo find specific information you can directly search the entire content of this site. An index is also available to aid navigation.\\nPlease enter your search request here:\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n© Environment Agency 1996. If you have any difficulties with this site please contact the Webmaster.  Don\\'t forget to register to be kept up to date with any changes\\n\\n\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5HRiKqjO9eyw"
      },
      "source": [
        "Thankfully BeautifulSoup makes it easy to remove script elements from the HTML. This time we get pure text, however, if we were to compare this with the original web page we see that text from menus is also included. This might be ok for an individual page but when performing text analysis across a site the menu items will be repeated for every page thereby creating unwanted noise."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "75mhxtNQ9EVK",
        "outputId": "d216f348-5063-4d4f-c2be-fee830078aad"
      },
      "source": [
        "ex_id = 3\n",
        "ex = examples[ex_id-1]\n",
        "page = requests.get(ukgwa_prefix + ex[1] + \"/\" + ex[0])\n",
        "soup = BeautifulSoup(page.content, 'html.parser')\n",
        "for s in soup.select('script'):\n",
        "    s.extract()\n",
        "soup.get_text().replace(\"\\n\",\" \").replace(\"  \",\" \")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"          Home | Office of Rail Regulation             Skip to content    Search Menu        About ORR  < Back  Who we are  < Back  The board  < Back  Committees  Register of interests  Board meeting minutes   Executive directors   What we do  < Back  Our functions  The law and our duties  < Back  Our duties  Railway regulatory law  UK law  EU law   Our vision and strategy  < Back  Corporate strategy  Health and safety strategy  < Back  Health and safety regulatory approach    How we work  < Back  Business plan  < Back  Impact assessments   Accountability  Sustainable development    Who we work with  < Back  Rail infrastructure  < Back  Mainline network  Underground railways  Light rail and tramways  Minor and heritage railways   Government  < Back  Department for Transport  Transport for London  Transport Scotland  Welsh Government  Department of Regional Development, Northern Ireland  Passenger transport executives  The European Railway Agency  Channel Tunnel   Safety bodies  < Back  Health and Safety Executive  Rail Accident Investigation Branch  Rail Safety and Standards Board  British Transport Police  Railway Industry Health and Safety Advisory Committee   Expert advisors  < Back  Consumer expert panel  Expert advisory panel   Industry organisations  < Back  Network Rail  Train operator companies  Freight operator companies  Rolling stock companies  Rail Delivery Group   Memoranda of Understandings  UK Regulators' Network   How we are funded  Open rail  < Back  Transparency programme  The Rail industry in Britain  < Back  Railway funding in Britain    ORR Open Data  < Back  Corporate data  < Back  Workforce management information  Spending figures  Consultancy spend  ORR organogram and data sets  Government Procurement Card figures  Tax arrangements  Board expenses  Civil service people survey   Freedom of information  < Back  Publication scheme  Making a request for information  Data protection  Environmental Information Regulations 2004    Supplying ORR  < Back  Minimum level of competition  Our policy on working with SMEs  Online e-tendering system  Policy on public expenditure transparency   Working at ORR  Contact us  < Back  Our offices  Complaints  < Back  Network Rail complaints  Complaints about ORR  Customer correspondence team     What & how we regulate  < Back  Health and safety  < Back  Regulation and certification  < Back  ROGS  < Back  Impact of ROGS  Compliance with ROGS  Dispute resolution  Safety critical work  Entities in charge of maintenance  < Back  Certificates issued   Safety verification  Safety certificates and authorisation  < Back  Mainline certificates  Non-mainline certificates   Exclusions from mainline railway requirements  Annual safety reports   Safety approvals under orders and private acts  European railway safety legislation  < Back  Common safety methods   Interoperability  Train driving licences & certificates  Rail vehicle accessibility  Railway Safety Regulations 1997  Railway Safety Regulations 1999   Monitoring and reporting  < Back  Health and safety report 2013  < Back  Director overview  Legislative framework  The regulator's view  < Back  Network Rail  Railway operators   The wider health and safety landscape  Performance in 2012-13  Enforcement activity  Health and safety plans for 2013-14   Occupational health quarterly updates  Occupational health assessment  Occupational health progress reports  Work and health effects   Investigating incidents  < Back  Reporting RIDDOR incidents  < Back  Types of Incidents  Reporting occupational diseases   How we investigate incidents  Handling RAIB recommendations  < Back  Responses to RAIB reports 2013  Responses to RAIB reports 2012  Responses to RAIB reports 2011    Enforcement  < Back  Enforcement powers  Enforcement policy  Enforcement penalties  Enforcement action taken  < Back  Prosecutions  Occupational health recent formal enforcement    Guidance and research  < Back  Occupational health guidance  < Back  Advice and guidance  Managing work related stress  < Back  What rail employers need to do  Working together  Sharing good practice  Further help and guidance   Rail manager competence  < Back  Why it is necessary  Law requirements  Training needs for managers  Good practice guidance  Achieving good practice  Further resources   Case studies   Worker safety  < Back  Working patterns - fatigue  Asbestos  < Back  Asbestos law    Infrastructure safety  < Back  Level crossings  < Back  Types of level crossings  Managing risks at level crossings  Using level crossings safely  Level crossings policy  Level crossings law  Safety performance   Signals passed at danger  Train protection  < Back  ERTMS  Train protection system exemptions   Mark I rolling stock  < Back  Exemptions   Road/rail interface sites  Route crime   Passenger safety  < Back  Station safety  Crowding on trains  < Back  Types of crowding    Risk management  Tramway technical guidance   Occupational health  < Back  Presentations and events    Network Rail regulation  < Back  How we regulate Network Rail  < Back  Accountability of Network Rail  Periodic review 2013  < Back  PR13 guide  < Back  About PR13  Our decisions  2014-19 regulation  Rail industry costs  Key stages of the review  Key dates   PR13 consultations  Workshops  < Back  Efficient expenditure workshop  Financial issues workshop  Track access charges workshop  Incentives workshop  Possessions regime workshop  Performance regime workshop  PR13 launch events   PR13 publications  < Back  Final determination  PR13 consultants' reports  Initial industry plans  Setting the financial and incentive framework for Network Rail in control period 5 (CP5)  PR13 legal notices  Advice to Ministers  Efficiency benchmarking of Network Rail  Key publications by stakeholders  Speeches and presentations  CP5 price lists   Success criteria   Control period 4  < Back  Periodic review 2008  < Back  PR08 documents  PR08 consultants' reports  PR08 notices  Best practice studies   CP4 delivery plan  CP4 enhancement projects    Monitoring performance  < Back  Network Rail Monitor  Efficiency and finance assessment  < Back  Annex to the 2013 report  Annex to the 2012 report   Underspend and efficiency   Network licence  < Back  Network licence conditions  Consents issued to Network Rail  < Back  Financial ring-fence  Interests in railway vehicles  Land disposal cases  < Back  2013 cases  2012 cases    Network management  Asset management  Land disposal policy  Ring-fencing  Route utilisation strategies  Regulatory accounts  Independent reporters  < Back  Related links  < Back  Efficiency and financial assessment of Network Rail     Enforcement of the licence  < Back  Licence enforcement taken  < Back  Overruns documentation  Infrastructure capability documentation  Portsmouth resignalling documentation   Enforcement relating to operational performance  Enforcement relating to Integrated Train Planning System    High Speed 1 (HS1)  < Back  Access to HS1  < Back  Criteria and procedures  Application forms and template track access contracts  Applications, decisions and consolidated agreements  The HS1 Network Code  HS1 access dispute resolution rules and appeals   Annual reports on HS1 Ltd  HS1 periodic reviews and access charges reviews  < Back  HS1 PR14  HS1 access charges reviews   HS1 monitoring and enforcement  < Back  Monitoring and reporting on HS1  Network regulation / asset management  Enforcement of HS1 limited    Competition and consumers  < Back  Competition issues  < Back  How to complain under competition law  Complaints about rail fares and car park charges  Current investigations  Competition complaints investigated  < Back  Real Time Train Information  NTM Sales  Virgin fares  DB Schenker    Consumer issues  Market studies   Track access  < Back  Criteria & procedures  Track access process  < Back  How to apply for track access  < Back  Access for passenger operators  Access for freight operators  Access for freight customers  Freight terminals  Track access options  Connection contracts  Access exemptions   Current track access applications  < Back  New contracts (section 17 and 18)  Amendments to contracts (Section 22 and 22a)   Track access decisions  < Back  New disputed contracts (Section 17)  New agreed contracts (Section 18)  Agreed amendments to contracts (Section 22)  Disputed amendment to contracts (Section 22a)  Other contract decisions  Decisions library  < Back  Section 17 decisions 2012-14  Section 18 decisions 2012-13  Section 22 decisions 2013  Section 22 decisions 2012  Section 22a decisions 2012-13  Other track access decisions 2012-13    Consolidated agreements   Current work  < Back  Industry reform  West Coast Main Line  East Coast Main Line   The Network Code  < Back  Current appeals  Determined appeals  Modifications   Policies   Station and depot access  < Back  Station access  Depot access  Making an application  General approvals  Template documentation  Station and depot closures  Station access applications and decisions  Depot access applications and decisions   Licensing  < Back  Licensing railway operators  < Back  Background  Licence obligations  < Back  3rd party liability   How to apply  Model licences  Model licence exemptions  Current licences  < Back  A - E  F - J  K - R  S - Z   Licence exemptions  < Back  A - D  E - K  L - S  T - Z   Licence consultations   Train driving licences & certificates   Permissions & approvals  Investments   Info for passengers  < Back  Fares  Station car park charges  Safety  < Back  Platform gaps  On-train crowding   Service disruption  Train journey information  Passengers with disabilities  High speed rail (HS2)   Publications  < Back  Publications search  Publications A-Z  Corporate publications  Policies and statements  < Back  Health and safety   Reports  < Back  Health and safety   Notices  < Back  Licence consultations  Legal notices  < Back  Improvement notices  < Back  Improvement notices 2013  Improvement notices 2012   Prohibition notices  < Back  Prohibition notices 2013  Prohibition notices 2012   PR13 legal notices  PR08 notices  Penalty notices    Correspondence  Guidance  < Back  Health and safety   Research and studies  For government  Public Register   Consultations  < Back  Policy consultations  < Back  Open consultations  Closed consultations  < Back  Closed consultations 2014  Closed consultations 2013  Closed consultations 2012  Closed consultations 2011   PR13 consultations  ORR responses to consultations issued by other organisations   Licence consultations  Access consultations  < Back  Station access applications and decisions  Depot access applications and decisions  Current track access applications  < Back  New contracts (section 17 and 18)  Amendments to contracts (Section 22 and 22a)   Track access decisions  < Back  New disputed contracts (Section 17)  New agreed contracts (Section 18)  Agreed amendments to contracts (Section 22)  Disputed amendment to contracts (Section 22a)  Other contract decisions  Decisions library  < Back  Section 17 decisions 2012-14  Section 18 decisions 2012-13  Section 22 decisions 2013  Section 22 decisions 2012  Section 22a decisions 2012-13  Other track access decisions 2012-13      Statistics  < Back  About our data portal  Published statistics  < Back  Statistical releases  Station usage estimates  Complaints data  < Back  Background to complaints data   Archived data  < Back  Archived statistical releases  NRT quarterly summary  ORR archived National rail trends publications  Archived SRA rail trends publications  Health and safety statistics 2002-2008    Code of Practice  < Back  Revisions log   Publication dates  User engagement  Contact us   News & media  < Back  Press releases  < Back  2014  2013  2012  2011  2010   Email alerts  < Back  2014  2013  2012  2011  2010   Speeches and presentations  < Back  2014  < Back  Rail passenger complaints handling: from process to culture 13 March 2014 workshop  Helping people use our railways: empowerment and awareness Disabled People’s Protection Policies 13 March 2014 Workshop   2013  2012  2011  2010      About ORR  Who we are  What we do  Who we work with  How we are funded  Open rail  ORR Open Data  Supplying ORR  Working at ORR  Contact us   What & how we regulate  Health and safety  Network Rail regulation  High Speed 1 (HS1)  Competition and consumers  Track access  Station and depot access  Licensing  Permissions & approvals  Investments   Info for passengers  Fares  Station car park charges  Safety  Service disruption  Train journey information  Passengers with disabilities  High speed rail (HS2)   Publications  Publications search  Publications A-Z  Corporate publications  Policies and statements  Reports  Notices  Correspondence  Guidance  Research and studies  For government  Public Register   Consultations  Policy consultations  Licence consultations  Access consultations   Statistics  About our data portal  Published statistics  Code of Practice  Publication dates  User engagement  Contact us   News & media  Press releases  Email alerts  Speeches and presentations            Compensation and refunds rights Three quarters of rail passengers are unaware of their compensation and refunds rights when trains are delayed or cancelled, reveals our study Read more   Periodic review 2013 A periodic review provides a major opportunity to drive through a step change in industry performance and efficiency Read our guide to the periodic review   ORR's final determination Passengers are at the heart of final plans for Britain's railways over the next five years Read our final determination     Latest news  \\r       02\\r     \\r       Apr 14\\r      Passenger experience report  \\r       20\\r     \\r       Mar 14\\r      Regulator publishes latest statistics on rail passenger complaints  \\r       19\\r     \\r       Mar 14\\r      UK Regulators launch new network to bring cross-sector regulation closer together      About ORR Who we are What we do Open rail    What and how we regulate Health and safety Network Rail Competition and consumers    Statistics Data portal Statistical releases Publication dates    Back to top      About ORR Freedom of informationGlossaryInvestor in peopleWorking at ORR Other ORR sites Data portalPublic registerVacancies About this site Accessibility statementCookies policySite mapTerms and conditions Contact us Accident reportingGeneral enquiriesMediaTell us what you think  Follow us  Twitter  LinkedIn  YouTube  RSS  Email   Download the latest version of Adobe Reader  © Copyright 2014 Office of Rail Regulation \\r         All content is available under the Open Government Licence v2.0, except where otherwise stated\\r           \""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4qh1t7M-E8v"
      },
      "source": [
        "A more sophisticated approach to extracting content is to use a boilerplate removal tool, such as boilerpy3. This tool will strip out all of the menus and only return the textual content of the page.\n",
        "\n",
        "The tool needs to be used with care though. While the first example looks great, we see that example 2 returns nothing at all, while example 3 returns a copyright notice. This is because home pages are entry points to a site and are often to help find pages within the site. They are therefore light on content and heavy on boilerplate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "I2OzwdYh9_1B",
        "outputId": "ba3fd78b-7443-4351-acf7-8fa2104996f4"
      },
      "source": [
        "ex_id = 2\n",
        "ex = examples[ex_id-1]\n",
        "page = requests.get(ukgwa_prefix + ex[1] + \"/\" + ex[0])\n",
        "soup = BeautifulSoup(page.content, 'html.parser')\n",
        "extractor = extractors.ArticleExtractor()\n",
        "content = extractor.get_content(str(soup))\n",
        "content"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "''"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9Q3bkHa2zDI"
      },
      "source": [
        "## Auto-generating website summaries\n",
        "\n",
        "In the first notebook we looked at the varios descriptions available in Discovery and the A-Z list. The most descriptive of these was the Administrative History from Discovery. Currently this only exists for around one-third of website. The content of the web archive is open and fully indexed so it can be searched but one feature of a good catalogue description is that it provides an overview of a document to let the user know whether that document is worth opening or requesting from the reading room.\n",
        "\n",
        "The great thing about computational access is that in the absence of a catalogue summary we can automate the creation of our own. Using some of the code from earlier to find links, combined with the boilerplate removal tool, we can extract the content from pages linked to on the home page of a site, which are part of that site.\n",
        "\n",
        "Then we can apply a machine learning algorithm called Document Summarisation (from the gensim NLP library) to summarise all of the pages into a short paragraph.\n",
        "We will try it for one of the earlier examples. First, we crawl each linked to page and extract the content into a list:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHwoS1GPDzim"
      },
      "source": [
        "We can compare some of these results with the Administrative History from the first notebook. First we need to set up the data including extracting from the Discovery API, as in notebook 1.\n",
        "\n",
        "<b>Estimated running time 40 seconds</b>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NCDap3F1Dyie",
        "outputId": "928d671e-6f5f-4b4b-fab0-9cde7d2718ca"
      },
      "source": [
        "W = UKGWAIndex()\n",
        "W.indexfromweb()\n",
        "W.discoveryfromfile(data_folder + 'ukgwa_catrefs.txt')\n",
        "disco_web_lookup = {}\n",
        "ex_urls = [ex[0] for ex in examples]\n",
        "for w in W:\n",
        "    url = W.get_field(w, 'URL')\n",
        "    if url[-1] == \"/\":\n",
        "        url = url[:-1]\n",
        "    if url in ex_urls:\n",
        "        catref = W.get_field(w, 'CATREF')\n",
        "        disco_web_lookup[catref] = w\n",
        "\n",
        "D = DiscoSearch()\n",
        "D.add_entry('web AND snapshots')\n",
        "example_admin = {}\n",
        "for d in D:\n",
        "    if D.get_field(d, 'reference') in disco_web_lookup:\n",
        "        web_ref = disco_web_lookup[D.get_field(d, 'reference')]\n",
        "        url = W.get_field(web_ref, 'URL')\n",
        "        if url[-1] == \"/\":\n",
        "            url = url[:-1]\n",
        "        example_admin[W.get_field(web_ref, 'URL')] = D.get_field(d, 'adminHistory')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "http://www.investorsinpeople.co.uk/Pages/Home.aspx\n",
            "http://www.professionalstandards.org.uk\n",
            "http://www.salt.gov.uk\n",
            "http://www.ukbms.org\n",
            "http://www.esero.org.uk\n",
            "IAID: C17986 URL: http://www.investorsinpeople.co.uk/Pages/Home.aspx Data: Investors in People UK was formed in 1993 to provide business leadership and development for the investors in people standard, and to lead and undertake national promotion of the standard. The snapshots in this series begin in the period before June 2007 when it was the responsibility of the Department for Education and Skills, before becoming a non-departmental public body which receives funding from the Department for Business, Innovation and Skills (BIS).\n",
            "IAID: C11689433 URL: http://www.esero.org.uk/ Data: ESERO-UK, also known as the UK Space Education Office, aims to promote the use of space to enhance and support the teaching and learning of Science, Technology, Engineering and Mathematics (STEM) in schools and colleges throughout the UK. ESERO-UK is funded by the European Space Agency (ESA) and the Department for Education.\n",
            "IAID: C13497803 URL: http://www.professionalstandards.org.uk/ Data: The Professional Standards Authority for Health and Social Care oversees statutory bodies that regulate health and social care professionals in the UK. The Authority assesses their performance, conducts audits, scrutinises their decisions and reports to Parliament. The Authority also sets standards for organisations holding voluntary registers for health and social care occupations and accredit those that meet them. The Authority was previously known as the Council for Healthcare Regulatory Excellence (CHRE).\n",
            "IAID: C16934 URL: http://www.salt.gov.uk/ Data: \n",
            "IAID: C17571 URL: http://www.ukbms.org/ Data: The UK Butterfly Monitoring Scheme (UKBMS) is run as a partnership between the Centre for Ecology and Hydrology (CEH), Butterfly Conservation (BC) and the Joint Nature Conservation Committee (JNCC).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jLYAUmGOAIA"
      },
      "source": [
        "This code performs the crawling of the pages linked to from the home page (within the same site). It then summarises each page using the summarization function from gensim, and stores a list of the summaries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xUQueF9kr1gu",
        "outputId": "2941d235-d5f9-473d-b7f8-487ba46b509d"
      },
      "source": [
        "ex_id = 3\n",
        "ex = examples[ex_id-1]\n",
        "page = requests.get(ukgwa_prefix + ex[1] + \"/\" + ex[0])\n",
        "soup = BeautifulSoup(page.content, 'html.parser')\n",
        "\n",
        "print(\"Links from\", ex[1] + \"/\" + ex[0])\n",
        "links = soup.find_all('a')\n",
        "summaries = []\n",
        "\n",
        "\n",
        "for ln in links:\n",
        "    if ln.has_attr('href'):\n",
        "        href = ln['href']\n",
        "        if href[0] == \"#\":\n",
        "            continue\n",
        "        if href[0:2] == \"./\":\n",
        "            href = href[2:]\n",
        "        parent_url = UKGWAurl(ukgwa_prefix + ex[1] + \"/\" + ex[0])\n",
        "        link_url = UKGWAurl(href, parent_url)\n",
        "        if link_url.get_domain() != parent_url.get_domain():\n",
        "            continue\n",
        "        print(\"\\t\", href, link_url)\n",
        "        page = requests.get(link_url)\n",
        "        soup = BeautifulSoup(page.content, 'html.parser')\n",
        "        extractor = extractors.ArticleExtractor()\n",
        "        content = extractor.get_content(str(soup))\n",
        "        summary = summarizer.summarize(content, ratio = 0.2)\n",
        "        summaries.append(summary)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Links from 20140402194840/http://www.ukbms.org\n",
            "\t Default.aspx https://webarchive.nationalarchives.gov.uk/20140402194840/http://www.ukbms.org/Default.aspx\n",
            "\t involved.aspx https://webarchive.nationalarchives.gov.uk/20140402194840/http://www.ukbms.org/involved.aspx\n",
            "\t wcbs.aspx https://webarchive.nationalarchives.gov.uk/20140402194840/http://www.ukbms.org/wcbs.aspx\n",
            "\t indicators.aspx https://webarchive.nationalarchives.gov.uk/20140402194840/http://www.ukbms.org/indicators.aspx\n",
            "\t KeyFindings.aspx https://webarchive.nationalarchives.gov.uk/20140402194840/http://www.ukbms.org/KeyFindings.aspx\n",
            "\t news.aspx https://webarchive.nationalarchives.gov.uk/20140402194840/http://www.ukbms.org/news.aspx\n",
            "\t reportsAndPublications.aspx https://webarchive.nationalarchives.gov.uk/20140402194840/http://www.ukbms.org/reportsAndPublications.aspx\n",
            "\t resources.aspx https://webarchive.nationalarchives.gov.uk/20140402194840/http://www.ukbms.org/resources.aspx\n",
            "\t Specieslist.aspx https://webarchive.nationalarchives.gov.uk/20140402194840/http://www.ukbms.org/Specieslist.aspx\n",
            "\t map.aspx https://webarchive.nationalarchives.gov.uk/20140402194840/http://www.ukbms.org/map.aspx\n",
            "\t About.aspx https://webarchive.nationalarchives.gov.uk/20140402194840/http://www.ukbms.org/About.aspx\n",
            "\t Methods.aspx https://webarchive.nationalarchives.gov.uk/20140402194840/http://www.ukbms.org/Methods.aspx\n",
            "\t Obtaining.aspx https://webarchive.nationalarchives.gov.uk/20140402194840/http://www.ukbms.org/Obtaining.aspx\n",
            "\t Links.aspx https://webarchive.nationalarchives.gov.uk/20140402194840/http://www.ukbms.org/Links.aspx\n",
            "\t Contacts.aspx https://webarchive.nationalarchives.gov.uk/20140402194840/http://www.ukbms.org/Contacts.aspx\n",
            "\t wcbs.aspx https://webarchive.nationalarchives.gov.uk/20140402194840/http://www.ukbms.org/wcbs.aspx\n",
            "\t about.aspx https://webarchive.nationalarchives.gov.uk/20140402194840/http://www.ukbms.org/about.aspx\n",
            "\t Methods.aspx https://webarchive.nationalarchives.gov.uk/20140402194840/http://www.ukbms.org/Methods.aspx\n",
            "\t Obtaining.aspx https://webarchive.nationalarchives.gov.uk/20140402194840/http://www.ukbms.org/Obtaining.aspx\n",
            "\t Downloads\\Wider_Countryside\\Newsletter_WCBS2013.pdf https://webarchive.nationalarchives.gov.uk/20140402194840/http://www.ukbms.org/Downloads\\Wider_Countryside\\Newsletter_WCBS2013.pdf\n",
            "\t https://webarchive.nationalarchives.gov.uk/20140402194840/http://www.ukbms.org/mydata https://webarchive.nationalarchives.gov.uk/20140402194840/http://www.ukbms.org/mydata\n",
            "\t KeyFindings.aspx https://webarchive.nationalarchives.gov.uk/20140402194840/http://www.ukbms.org/KeyFindings.aspx\n",
            "\t wider_countryside_reporting_visit.aspx https://webarchive.nationalarchives.gov.uk/20140402194840/http://www.ukbms.org/wider_countryside_reporting_visit.aspx\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1hKs8N6f5IKX"
      },
      "source": [
        "Next we concatenate all of those summaries together into one big document and summarise that. You can experiment with the length of the summary using the word count parameter but 100 words produces a nice summary which provides an overview of the web site.\n",
        "\n",
        "It is not a catalogue description, it is built from heuristics not knowledge, but it is a powerful technique and this notebook environment provides an ideal playground to try it out.\n",
        "\n",
        "Run this code to see the result and to compare it against the human curated Administrative History:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-OC-ZkNi1gjv",
        "outputId": "e081a792-4ad3-43fe-d377-4ea38478af12"
      },
      "source": [
        "all_summaries = \"\\n\".join(summaries)\n",
        "print(ex[0])\n",
        "print(\"\")\n",
        "print(\"Auto summary\")\n",
        "print(summarizer.summarize(all_summaries, word_count=100))\n",
        "print(\"\")\n",
        "print(\"Admin History\")\n",
        "if ex[0] in example_admin:\n",
        "    print(example_admin[ex[0]])\n",
        "else:\n",
        "    print(example_admin[ex[0] + \"/\"])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "http://www.ukbms.org\n",
            "\n",
            "Auto summary\n",
            "The Wider Countryside Butterfly Survey (WCBS) is the main scheme for monitoring population changes of the UK's common and widespread butterflies.\n",
            "The Wider Countryside Butterfly Survey (WCBS) is the main scheme for monitoring population changes of the UK's common and widespread butterflies.\n",
            "The WCBS is the most comprehensive UK-wide survey of insect abundance to use a robust random sampling framework and is important in assessing the changing status of butterfly species in the wider countryside and in providing an indicator of the health of nature.\n",
            "\n",
            "Admin History\n",
            "The UK Butterfly Monitoring Scheme (UKBMS) is run as a partnership between the Centre for Ecology and Hydrology (CEH), Butterfly Conservation (BC) and the Joint Nature Conservation Committee (JNCC).\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}